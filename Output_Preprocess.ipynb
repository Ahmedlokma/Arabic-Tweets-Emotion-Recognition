{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujx2GHMfhGln",
        "outputId": "94d9546e-489c-4187-889c-6f1d5675b215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Preprocessing***"
      ],
      "metadata": {
        "id": "omWlRpuldZZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyarabic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWF9wTheOiH",
        "outputId": "759040fd-380b-425f-fccf-134efbfdf158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "from pyarabic.araby import strip_tashkeel, strip_tatweel"
      ],
      "metadata": {
        "id": "vYzMUinteUkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "-DXM2L2Rdi6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9-SncvndYzw",
        "outputId": "27bbe0ac-9c35-4761-eda3-f1b92ae0b08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_tweet(tweet):\n",
        " \n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "  \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_punctuation(tokens):\n",
        "    # Remove punctuation from the tokens using a regular expression\n",
        "    no_punct_tokens = [re.sub(r'[^a-zA-Z؀-ۿ]', '', token) for token in tokens]\n",
        "    \n",
        "    # Remove any empty tokens using a loop\n",
        "    new_tokens = []\n",
        "    for token in no_punct_tokens:\n",
        "        if token:\n",
        "            new_tokens.append(token)\n",
        "    no_punct_tokens = new_tokens\n",
        "    \n",
        "    return no_punct_tokens"
      ],
      "metadata": {
        "id": "je6DSVOvd2r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(tweet, stopword_file):\n",
        "    with open(stopword_file, 'r', encoding='utf-8') as f:\n",
        "        stop_words = f.read().splitlines()\n",
        "\n",
        "    # Tokenize the tweet\n",
        "    tokens = tweet.split()\n",
        "\n",
        "    no_stopword_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            no_stopword_tokens.append(token)\n",
        "    \n",
        "    # Join the tokens back into a processed tweet\n",
        "    processed_tweet = \" \".join(no_stopword_tokens)\n",
        "    \n",
        "    return processed_tweet\n",
        "\n"
      ],
      "metadata": {
        "id": "d94quU92d7Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "   \n",
        "    stemmer = ISRIStemmer()\n",
        "    \n",
        "    exclude_list = ['قلب', 'رحمة', 'أمل', 'حزن', 'سعادة', 'جمال', 'شجاعة', 'تفاؤل', 'يقين', 'تضامن',\n",
        "                    'حرية', 'عدالة', 'إنسانية', 'عزيمة', 'وفاء', 'إخلاص', 'صدق', 'شفاء', 'دعاء', 'الله']\n",
        "    \n",
        "    stemmed_tokens = [stemmer.stem(token) if token not in exclude_list else token for token in tokens]\n",
        "    return stemmed_tokens"
      ],
      "metadata": {
        "id": "XtrPLD0qd-vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet2(tweet):\n",
        "    # Remove mentions (@username)\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9_]+\", \"\", tweet)\n",
        "    # Remove retweets (RT)\n",
        "    tweet = re.sub(r\"RT\\s+\", \"\", tweet)\n",
        "    # Remove URLs (http or https)\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", \"\", tweet)\n",
        "    # Remove any remaining non-Arabic characters\n",
        "    tweet = re.sub(r\"[^؀-ۿ]+\", \" \", tweet)\n",
        "    # Remove extra whitespace\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet.strip())\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "TfcBEcWZeC2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = normalize_hamza(tweet)\n",
        "    tweet = strip_tatweel(tweet)\n",
        "    tweet = strip_tashkeel(tweet)\n",
        "\n",
        "\n",
        "    tweet = re.sub(\"[ًٌٍَُِّْٰ]\", \"\", tweet)\n",
        "    tweet = re.sub(\"[إأٱآا]\", \"ا\", tweet)\n",
        "    tweet = re.sub(\"ى\", \"ي\", tweet)\n",
        "    tweet = re.sub(\"ؤ\", \"ء\", tweet)\n",
        "    tweet = re.sub(\"ئ\", \"ء\", tweet)\n",
        "    tweet = re.sub(\"ة\", \"ه\", tweet)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', tweet)\n",
        "    tweet = tweet.lower()\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "iXJMqP2BeHVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_hamza(text):\n",
        "    \"\"\"Normalize Alef with Hamza Above and Alef with Hamza Below to Alef\"\"\"\n",
        "    text = re.sub(\"[أإآا]\", \"ا\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "GA0UZ_i0eK0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF Feature Extraction**"
      ],
      "metadata": {
        "id": "x3ynAV8ZkIw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the TfidfVectorizer object from the saved file\n",
        "tfidf_vectorizer = joblib.load('/content/drive/MyDrive/tfidf_vectorizer.sav')\n"
      ],
      "metadata": {
        "id": "90n0CVqgkPr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Model**"
      ],
      "metadata": {
        "id": "6zZn37MyqDax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = joblib.load('/content/drive/MyDrive/svm_model.sav')"
      ],
      "metadata": {
        "id": "I9vASdFRqHf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Svm **"
      ],
      "metadata": {
        "id": "-QRR7JbVqUgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"السحب الليلة على الايفون .. رتويت للمرفقة وطبق الشروط 👇\"\n",
        "no_stopword_tokens = remove_stopwords(tweet, '/content/list.txt')\n",
        "print(no_stopword_tokens)\n",
        "tweet = clean_tweet(no_stopword_tokens)\n",
        "tweet = clean_tweet2(tweet)\n",
        "tweet = remove_emojis(tweet)\n",
        "tokens = tokenize_tweet(tweet)\n",
        "no_punct_tokens = remove_punctuation(tokens)\n",
        "cleaned_tweet = ' '.join(no_punct_tokens)\n",
        "print(cleaned_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB3HwYycqX38",
        "outputId": "5f1db41b-1e5a-4afa-dc0d-40dccad9ee67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "السحب الليلة الايفون .. رتويت للمرفقة وطبق الشروط 👇\n",
            "السحب الليله الايفون رتويت للمرفقه وطبق الشروط\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_features = tfidf_vectorizer.transform([cleaned_tweet])\n",
        "\n",
        "# Make prediction using the loaded SVM model\n",
        "prediction = svm_model.predict(tweet_features)\n",
        "print(prediction)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnWEnv5trDRe",
        "outputId": "80726fa2-8f6b-4228-a636-18deab54ff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Trees**"
      ],
      "metadata": {
        "id": "CekMrbGX0oye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_model = joblib.load('/content/drive/MyDrive/dtc_model.sav')"
      ],
      "metadata": {
        "id": "OfAYYBJOt4iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_features = tfidf_vectorizer.transform([cleaned_tweet])\n",
        "\n",
        "# Make prediction using the loaded SVM model\n",
        "prediction = dtc_model.predict(tweet_features)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyMdM4UyuqcK",
        "outputId": "46c041e8-9dfb-4262-e4c8-eab2ca165df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "64M7-JbH0kKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_model = joblib.load('/content/drive/MyDrive/nb_model.sav')"
      ],
      "metadata": {
        "id": "4aqnZ0skzStb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_features = tfidf_vectorizer.transform([cleaned_tweet])\n",
        "\n",
        "# Make prediction using the loaded SVM model\n",
        "prediction = nb_model.predict(tweet_features)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kes7kClC0EF5",
        "outputId": "d59ef630-87bd-41c8-bbc2-a549ea33dd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive']\n"
          ]
        }
      ]
    }
  ]
}